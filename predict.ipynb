{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import label_loader\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('../bert-base-chinese')\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./channel-classifier-man\")\n",
    "model.to(device)\n",
    "# man_model = AutoModelForSequenceClassification.from_pretrained(\"./channel-classifier-man\")\n",
    "# man_model.to(device)\n",
    "# classifier = AutoModelForSequenceClassification.from_pretrained(\"../classification-yue/yue-classifier-can\")\n",
    "# classifier.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40  # took about 6GB of GPU memory\n",
    "label_idx = label_loader.load_label_index('labels.tsv')\n",
    "idx_label = {v:k for k,v in label_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "def text2token(txt):\n",
    "  d = tokenizer(txt,padding=\"max_length\", truncation=True,return_tensors='pt',max_length=64)\n",
    "  d.update((k,v[0]) for k,v in d.items())\n",
    "  return d\n",
    "def batchPredict(batch_text : list,model=model):\n",
    "    ret = []\n",
    "    tokens = [text2token(t) for t in batch_text]\n",
    "    d = DataLoader(tokens,batch_size=batch_size,shuffle=False)\n",
    "    datasets = tqdm(d)\n",
    "    for batch in datasets:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=mask)\n",
    "        logits = outputs.logits\n",
    "        ret= [*ret,*logits.tolist()]\n",
    "    return ret\n",
    "\n",
    "def predict(text,model=model):\n",
    "    token = tokenizer(text,return_tensors='pt')\n",
    "    inputs = token['input_ids'].to(device)\n",
    "    mask = token['attention_mask'].to(device)\n",
    "    outputs = model(input_ids=inputs, attention_mask=mask)\n",
    "    logits = outputs.logits\n",
    "    return logits.tolist()\n",
    "\n",
    "# def classify(batch_text):\n",
    "#     print(\"classifing\")\n",
    "#     ret = []\n",
    "#     tokens = [text2token(t) for t in batch_text]\n",
    "#     d = DataLoader(tokens,batch_size=batch_size,shuffle=False)\n",
    "#     datasets = tqdm(d)\n",
    "#     datasets.set_description(\"cantonese classification\")\n",
    "#     for batch in datasets:\n",
    "#         inputs = batch['input_ids'].to(device)\n",
    "#         mask = batch['attention_mask'].to(device)\n",
    "#         outputs = classifier(input_ids=inputs, attention_mask=mask)\n",
    "#         logits = outputs.logits\n",
    "#         ret= [*ret,*logits.argmax(-1).tolist()]\n",
    "#     return ret\n",
    "\n",
    "# def batchPredictByPrediction(batch_text: list,specific_classificaiton : int,spec_model):\n",
    "#     p_classification = classify(batch_text)\n",
    "#     filtered_batch = []\n",
    "#     for pc,t in zip(p_classification,batch_text):\n",
    "#         if pc == specific_classificaiton:\n",
    "#             filtered_batch.append(t)\n",
    "#     if specific_classificaiton == 1:\n",
    "#         return batchPredict(filtered_batch, spec_model)\n",
    "#     else:\n",
    "#         return batchPredict(filtered_batch, spec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_then_output_labels(labels):\n",
    "  dct = {}\n",
    "  ret = []\n",
    "  for label in labels:\n",
    "    for name,val in zip(label_idx.keys(),label):\n",
    "      dct[name] = val\n",
    "    ret.append(list(reversed(sorted(dct.items(),key=lambda x:x[1]))))\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('手機台', 8.261186599731445),\n",
       " ('時事台', 5.150082111358643),\n",
       " ('Apps台', 3.9472124576568604)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"上ifc要用安心出行嗎？\"\n",
    "labels = sort_then_output_labels (predict(text))\n",
    "torch.cuda.empty_cache()\n",
    "labels[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "p_val = sort_then_output_labels(batchPredict(batch_text))\n",
    "p_labels = [(p[0][0],p[1][0],p[2][0]) for p in p_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# single \n",
    "cnt = 0\n",
    "correct = 0\n",
    "with open('out_with_labels.tsv',encoding='utf-8') as f:\n",
    "  lines = f.readlines()\n",
    "for l in lines:\n",
    "  grp = str(l).strip().split('\\t')\n",
    "  p_label = sort_then_output_labels(predict(grp[1]))[0][0]\n",
    "  cnt += 1\n",
    "  correct += int(p_label == grp[0])\n",
    "print(f\"accuracy: {(correct / cnt) * 100:2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out_with_labels_ori.tsv',encoding='utf-8') as f:\n",
    "  lines = [str(l).strip().split('\\t') for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:44<00:00,  5.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# batch\n",
    "torch.cuda.empty_cache()\n",
    "p_val = sort_then_output_labels(batchPredict([grp[1] for grp in lines[300000:310000]]))\n",
    "eval_label = [grp[0] for grp in lines]\n",
    "p_labels = [(p[0][0],p[1][0],p[2][0]) for p in p_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in general: 17.630000%\n",
      "accuracy of 電訊台: 6.666667 count:45\n",
      "accuracy of 動漫台: 7.344633 count:177\n",
      "accuracy of 吹水台: 48.101266 count:1027\n",
      "accuracy of 時事台: 25.134553 count:1858\n",
      "accuracy of 財經台: 11.725453 count:1049\n",
      "accuracy of 娛樂台: 16.297609 count:1129\n",
      "accuracy of 體育台: 16.401590 count:1006\n",
      "accuracy of 遊戲台: 10.240964 count:166\n",
      "accuracy of 汽車台: 2.631579 count:114\n",
      "accuracy of 校園台: 4.237288 count:118\n",
      "accuracy of 感情台: 11.724138 count:580\n",
      "accuracy of World: 3.389831 count:59\n",
      "accuracy of 創意台: 15.151515 count:330\n",
      "accuracy of 影視台: 7.427056 count:377\n",
      "accuracy of 音樂台: 4.878049 count:246\n",
      "accuracy of 學術台: 14.159292 count:226\n",
      "accuracy of 上班台: 6.907895 count:304\n",
      "accuracy of 手機台: 0.943396 count:106\n",
      "accuracy of 寵物台: 3.389831 count:59\n",
      "accuracy of 飲食台: 8.013937 count:287\n",
      "accuracy of 玩具台: 4.761905 count:21\n",
      "accuracy of 健康台: 11.278195 count:133\n",
      "accuracy of 政事台: 9.523810 count:42\n",
      "accuracy of 房屋台: 8.270677 count:133\n",
      "accuracy of 站務台: 2.702703 count:37\n",
      "accuracy of 攝影台: 6.666667 count:30\n",
      "accuracy of 潮流台: 1.785714 count:56\n",
      "accuracy of 硬件台: 6.603774 count:106\n",
      "accuracy of 活動台: 0.000000 count:15\n",
      "accuracy of 旅遊台: 6.451613 count:31\n",
      "accuracy of Apps台: 2.000000 count:50\n",
      "accuracy of 軟件台: 5.882353 count:34\n",
      "accuracy of 直播台: 12.500000 count:16\n",
      "accuracy of 講故台: 3.030303 count:33\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "correct = 0\n",
    "grp_cnt = {}\n",
    "grp_correct = {}\n",
    "for e,p in zip(eval_label,p_labels):\n",
    "  if e in p:\n",
    "    correct += 1\n",
    "    grp_correct[e] = grp_correct.get(e,0) + 1\n",
    "  grp_cnt[e] = grp_cnt.get(e,0) + 1\n",
    "  cnt += 1\n",
    "print(f\"accuracy in general: {(correct / cnt) * 100:2f}%\")\n",
    "for label in grp_cnt.keys():\n",
    "  print(f\"accuracy of {label}: {grp_correct.get(label,0) / grp_cnt[label] * 100 :2f} count:{grp_cnt[label]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d607a3b8ecc050e73ae5a24a81ba0fefbd48bfd20415b0ce69a17a192c71bf1f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
